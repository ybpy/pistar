"""
Pistar æ•°æ®å¤„ç†æµç¨‹è„šæœ¬ï¼Œæ”¯æŒå¤æ‚çš„ reward/value/adv/epsilon è®¡ç®—

å¤„ç†æµç¨‹ï¼š
1. Pass 1: åŠ è½½æ‰€æœ‰æ•°æ®ï¼Œè½¬æ¢ rewardï¼Œè®¡ç®— value
2. Pass 2: è®¡ç®— advantagesï¼ŒæŒ‰ task ç»Ÿè®¡ advantages å¹¶è®¡ç®— epsilon (70% åˆ†ä½æ•°)
3. Pass 3: åŸºäº epsilon è®¡ç®— adv_indï¼Œå†™å…¥ LeRobot æ•°æ®é›†

æ³¨æ„ï¼š
- Value å–å€¼èŒƒå›´: [-1.0, 0.0]
- æ‰€æœ‰ value ä½¿ç”¨ --default_value å‚æ•°è®¾ç½®ï¼ˆé»˜è®¤ 0.0ï¼‰
- value_model_path åŠŸèƒ½å¾…å®ç°
- å¯é€šè¿‡ --default_adv_ind è·³è¿‡ adv è®¡ç®—ï¼Œç›´æ¥è®¾ç½®æ‰€æœ‰ adv_ind

Usage:
# è®¡ç®— adv å’Œ adv_ind
python examples/libero/pistar_data_processing.py \
    --data_dir /path/to/modified_libero_rlds \
    --default_value 0.0 \
    --n_steps 10

# è·³è¿‡ adv è®¡ç®—ï¼Œç›´æ¥è®¾ç½® adv_ind
python examples/libero/pistar_data_processing.py \
    --data_dir /path/to/modified_libero_rlds \
    --default_value 0.0 \
    --default_adv_ind positive

# ä½¿ç”¨ value æ¨¡å‹ï¼ˆå¾…å®ç°ï¼‰
python examples/libero/pistar_data_processing.py \
    --data_dir /path/to/modified_libero_rlds \
    --value_model_path /path/to/value_model.pth \
    --n_steps 10

# unbuffered è¾“å‡ºæ—¥å¿—ï¼ˆå®æ—¶æŸ¥çœ‹ï¼‰
python -u examples/libero/pistar_data_processing.py ... 
"""

import shutil
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple

from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME
from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
import tensorflow_datasets as tfds
import numpy as np
import tyro


REPO_NAME = "ybpy/libero_pistar"
RAW_DATASET_NAMES = [
    "libero_10_no_noops",
    "libero_goal_no_noops",
    "libero_object_no_noops",
    "libero_spatial_no_noops",
]


def transform_reward(original_reward: float, is_terminal: bool, is_last: bool, episode_length: int) -> float:
    """
    è½¬æ¢ reward çš„è§„åˆ™ï¼š
    1. å¦‚æœ is_terminal æˆ– is_last ä¸º True:
       - åŸå§‹ reward = 1.0 â†’ 0.0
       - åŸå§‹ reward = 0.0 â†’ -1.0
    2. å¦‚æœ is_terminal å’Œ is_last éƒ½ä¸º False (ä¸­é—´æ­¥éª¤):
       - reward = -1 / episode_length
    """
    if is_terminal or is_last:
        # è‡³å°‘ä¸€ä¸ªä¸º True
        if original_reward == 1.0:
            return 0.0
        else:  # original_reward == 0.0
            return -1.0
    else:
        # éƒ½ä¸º False (ä¸­é—´æ­¥éª¤): reward = -1 / episode_length
        return -1.0 / episode_length


def compute_value_placeholder(step_data: dict) -> float:
    """
    å ä½å‡½æ•°ï¼šè®¡ç®— value
    TODO: æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹æ¨ç†
    
    Args:
        step_data: åŒ…å« observation, action ç­‰çš„å­—å…¸
    
    Returns:
        value: é¢„æµ‹çš„çŠ¶æ€ä»·å€¼ (èŒƒå›´: [-1.0, 0.0])
    """
    # å½“å‰ä½¿ç”¨ 0.0 ä½œä¸ºé»˜è®¤å€¼
    # Value çš„å–å€¼èŒƒå›´é¢„å®šåœ¨ [-1.0, 0.0] ä¹‹é—´
    return 0.0


def compute_advantage(
    rewards: np.ndarray,
    values: np.ndarray,
    n_steps: int,
    gamma: float = 1.0
) -> np.ndarray:
    """
    è®¡ç®— advantage
    
    adv[t] = sum(rewards[t:t+N]) + value[t+N] - value[t]
    
    æ³¨æ„ï¼šå¯¹äº episode çš„æœ€å N ä¸ª stepï¼š
    - çª—å£ä¼šæˆªæ–­åˆ° episode ç»“æŸ
    - ä½¿ç”¨ episode æœ€åä¸€ä¸ª step çš„ value ä½œä¸º bootstrap
    - ä¾‹å¦‚ï¼št=95, T=100, N=10 æ—¶ï¼Œactual_steps=5
      adv[95] = sum(rewards[95:100]) + gamma^5 * value[99] - value[95]
    
    Args:
        rewards: shape (T,) çš„ reward æ•°ç»„
        values: shape (T,) çš„ value æ•°ç»„
        n_steps: N-step çª—å£å¤§å°
        gamma: æŠ˜æ‰£å› å­ (é»˜è®¤ 1.0)
    
    Returns:
        advantages: shape (T,) çš„ advantage æ•°ç»„
    """
    T = len(rewards)
    advantages = np.zeros(T, dtype=np.float32)
    
    for t in range(T):
        # è®¡ç®— N-step return (çª—å£ä¼šè‡ªåŠ¨æˆªæ–­åˆ° episode ç»“æŸ)
        n_step_return = 0.0
        actual_steps = min(n_steps, T - t)  # å®é™…èƒ½çœ‹åˆ°çš„æ­¥æ•°
        
        for i in range(actual_steps):
            n_step_return += (gamma ** i) * rewards[t + i]
        
        # æ·»åŠ  bootstrap value
        if t + n_steps < T:
            # æ­£å¸¸æƒ…å†µï¼šåŠ ä¸Š N æ­¥åçš„ value
            n_step_return += (gamma ** n_steps) * values[t + n_steps]
        else:
            # æœ€å N ä¸ª stepï¼šåŠ ä¸Š episode æœ€åä¸€ä¸ª step çš„ value
            n_step_return += (gamma ** actual_steps) * values[T - 1]
        
        # advantage = n_step_return - value[t]
        advantages[t] = n_step_return - values[t]
    
    return advantages


def main(
    data_dir: str,
    *,
    n_steps: int = 10,
    value_model_path: str | None = None,
    default_value: float = 0.0,
    default_adv_ind: str | None = None,
    epsilon_percentile: float = 70.0,
    repo_name: str = REPO_NAME,
    push_to_hub: bool = False,
):
    """
    Pistar æ•°æ®å¤„ç†å’Œè½¬æ¢
    
    Args:
        data_dir: RLDS æ•°æ®é›†è·¯å¾„
        n_steps: N-step advantage è®¡ç®—çš„çª—å£å¤§å°
        value_model_path: Value æ¨¡å‹è·¯å¾„ (å¯é€‰ï¼Œå¾…å®ç°)
        default_value: é»˜è®¤ value å€¼ (èŒƒå›´ [-1.0, 0.0]ï¼Œé»˜è®¤ 0.0)
        default_adv_ind: é»˜è®¤ adv_ind å€¼ ("positive" æˆ– "negative")ï¼Œ
                        å¦‚æœè®¾ç½®ï¼Œå°†è·³è¿‡ adv å’Œ epsilon è®¡ç®—
        epsilon_percentile: epsilon çš„åˆ†ä½æ•° (é»˜è®¤ 70.0)
        repo_name: è¾“å‡ºæ•°æ®é›†åç§°
        push_to_hub: æ˜¯å¦æ¨é€åˆ° HuggingFace Hub
    """
    
    print("=" * 80)
    print("ğŸš€ Pistar æ•°æ®å¤„ç†æµç¨‹")
    print("=" * 80)
    print(f"N-step window: {n_steps}")
    print(f"Default value: {default_value}")
    if default_adv_ind:
        print(f"Default adv_ind: {default_adv_ind} (è·³è¿‡ adv è®¡ç®—)")
    else:
        print(f"Epsilon percentile: {epsilon_percentile}%")
    print(f"Output repo: {repo_name}")
    
    # ========================================================================
    # Pass 1: åŠ è½½æ‰€æœ‰æ•°æ®ï¼Œè½¬æ¢ rewardï¼Œè®¡ç®— value
    # ========================================================================
    print("\n" + "=" * 80)
    print("ğŸ“Š Pass 1: åŠ è½½æ•°æ®å¹¶è®¡ç®— reward/value")
    print("=" * 80)
    
    # æ£€æŸ¥ value æ¨¡å‹
    if value_model_path:
        print(f"âš ï¸  Value model loading not yet implemented")
        print(f"    Using default value: {default_value}")
    else:
        print(f"ğŸ“Œ Using default value: {default_value}")
    
    # å­˜å‚¨æ‰€æœ‰ episodes çš„æ•°æ®
    all_episodes_data = []
    global_episode_idx = 0
    
    for dataset_name in RAW_DATASET_NAMES:
        print(f"\nğŸ”„ Processing: {dataset_name}")
        raw_dataset = tfds.load(dataset_name, data_dir=data_dir, split="train")
        
        for episode in raw_dataset:
            episode_data = {
                'steps': [],
                'task': None,
                'dataset_name': dataset_name,
                'global_episode_idx': global_episode_idx,
            }
            
            steps_list = list(episode['steps'].as_numpy_iterator())
            episode_length = len(steps_list)
            
            # è·å– task
            task = steps_list[0]['language_instruction']
            task = task.decode() if isinstance(task, bytes) else task
            episode_data['task'] = task
            
            for step_idx, step in enumerate(steps_list):
                # è½¬æ¢ reward
                original_reward = float(step['reward'])
                is_terminal = bool(step['is_terminal'])
                is_last = bool(step['is_last'])
                
                transformed_reward = transform_reward(
                    original_reward, is_terminal, is_last, episode_length
                )
                
                # è·å–æˆ–è®¡ç®— value
                if value_model_path:
                    # TODO: ä½¿ç”¨å®é™…æ¨¡å‹è®¡ç®— value
                    value = compute_value_placeholder(step)
                else:
                    value = default_value
                
                step_data = {
                    'observation': step['observation'],
                    'action': step['action'],
                    'original_reward': original_reward,
                    'transformed_reward': transformed_reward,
                    'value': value,
                    'is_terminal': is_terminal,
                    'is_last': is_last,
                    'step_idx': step_idx,
                }
                
                episode_data['steps'].append(step_data)
            
            all_episodes_data.append(episode_data)
            global_episode_idx += 1
            
            if global_episode_idx % 50 == 0:
                print(f"   Processed {global_episode_idx} episodes")
    
    print(f"\nâœ… Pass 1 complete: {global_episode_idx} episodes loaded")
    
    # ========================================================================
    # Pass 2: è®¡ç®— advantagesï¼Œç»Ÿè®¡æ¯ä¸ª task çš„ epsilon
    # ========================================================================
    task_epsilon = {}
    
    if not default_adv_ind:
        # åªæœ‰åœ¨éœ€è¦è®¡ç®— adv_ind æ—¶æ‰è®¡ç®— epsilon
        print("\n" + "=" * 80)
        print("ğŸ“ˆ Pass 2: è®¡ç®— advantages å¹¶ç»Ÿè®¡ epsilon")
        print("=" * 80)
        
        # æŒ‰ task æ”¶é›†æ‰€æœ‰ advantages
        task_advantages = defaultdict(list)
        
        for episode_data in all_episodes_data:
            task = episode_data['task']
            steps = episode_data['steps']
            
            # æå– rewards å’Œ values
            rewards = np.array([s['transformed_reward'] for s in steps], dtype=np.float32)
            values = np.array([s['value'] for s in steps], dtype=np.float32)
            
            # è®¡ç®— advantages
            advantages = compute_advantage(rewards, values, n_steps)
            
            # å­˜å‚¨ advantages åˆ° episode_data ä¸­ï¼Œä¾› Pass 3 ä½¿ç”¨
            episode_data['advantages'] = advantages
            
            # æ”¶é›†åˆ° task_advantages ä¸­
            task_advantages[task].extend(advantages.tolist())
        
        # è®¡ç®—æ¯ä¸ª task çš„ epsilon (åŸºäº advantages)
        for task, advantages in task_advantages.items():
            epsilon = np.percentile(advantages, epsilon_percentile)
            task_epsilon[task] = epsilon
            print(f"Task: {task[:50]}...")
            print(f"  Advantages count: {len(advantages)}")
            print(f"  Epsilon ({epsilon_percentile}%): {epsilon:.4f}")
        
        print(f"\nâœ… Pass 2 complete: {len(task_epsilon)} unique tasks")
    else:
        print("\n" + "=" * 80)
        print(f"â­ï¸  Pass 2: è·³è¿‡ (ä½¿ç”¨é»˜è®¤ adv_ind: {default_adv_ind})")
        print("=" * 80)
    
    # ========================================================================
    # Pass 3: è®¡ç®— adv_indï¼Œå†™å…¥æ•°æ®é›†
    # ========================================================================
    print("\n" + "=" * 80)
    print("ğŸ’¾ Pass 3: è®¡ç®— adv_ind å¹¶å†™å…¥æ•°æ®é›†")
    print("=" * 80)
    
    # æ¸…ç†å·²å­˜åœ¨çš„æ•°æ®é›†
    output_path = HF_LEROBOT_HOME / repo_name
    if output_path.exists():
        print(f"ğŸ—‘ï¸  Removing existing dataset at {output_path}")
        shutil.rmtree(output_path)
    
    # åˆ›å»º LeRobot æ•°æ®é›†
    dataset = LeRobotDataset.create(
        repo_id=repo_name,
        robot_type="panda",
        fps=10,
        features={
            "image": {
                "dtype": "image",
                "shape": (256, 256, 3),
                "names": ["height", "width", "channel"],
            },
            "wrist_image": {
                "dtype": "image",
                "shape": (256, 256, 3),
                "names": ["height", "width", "channel"],
            },
            "state": {
                "dtype": "float32",
                "shape": (8,),
                "names": ["state"],
            },
            "actions": {
                "dtype": "float32",
                "shape": (7,),
                "names": ["actions"],
            },
            "reward": {
                "dtype": "float32",
                "shape": (1,),
                "names": ["reward"],
            },
            "value": {
                "dtype": "float32",
                "shape": (1,),
                "names": ["value"],
            },
            "adv": {
                "dtype": "float32",
                "shape": (1,),
                "names": ["adv"],
            },
            "epsilon": {
                "dtype": "float32",
                "shape": (1,),
                "names": ["epsilon"],
            },
            "adv_ind": {
                "dtype": "string",
                "shape": (1,),
                "names": ["adv_ind"],
            },
        },
        image_writer_threads=10,
        image_writer_processes=5,
    )
    
    # å†™å…¥æ•°æ®
    total_steps = 0
    for ep_idx, episode_data in enumerate(all_episodes_data):
        task = episode_data['task']
        steps = episode_data['steps']
        
        if default_adv_ind:
            # ä½¿ç”¨é»˜è®¤ adv_indï¼Œè·³è¿‡ adv è®¡ç®—
            advantages = np.zeros(len(steps), dtype=np.float32)  # adv å…¨éƒ¨ä¸º 0
            epsilon = 0.0  # epsilon ä¹Ÿè®¾ä¸º 0
        else:
            # ä½¿ç”¨ Pass 2 ä¸­è®¡ç®—å¥½çš„ advantages å’Œ epsilon
            epsilon = task_epsilon[task]
            advantages = episode_data['advantages']
        
        # å†™å…¥æ¯ä¸ª step
        for step_idx, step_data in enumerate(steps):
            adv = advantages[step_idx]
            
            if default_adv_ind:
                adv_ind = default_adv_ind
            else:
                adv_ind = "positive" if adv > epsilon else "negative"
            
            dataset.add_frame({
                "image": step_data['observation']['image'],
                "wrist_image": step_data['observation']['wrist_image'],
                "state": step_data['observation']['state'],
                "actions": step_data['action'],
                "task": task,
                "reward": np.array([step_data['transformed_reward']], dtype=np.float32),
                "value": np.array([step_data['value']], dtype=np.float32),
                "adv": np.array([adv], dtype=np.float32),
                "epsilon": np.array([epsilon], dtype=np.float32),
                "adv_ind": adv_ind,
            })
            total_steps += 1
        
        dataset.save_episode()
        
        if (ep_idx + 1) % 50 == 0:
            print(f"   Written {ep_idx + 1}/{len(all_episodes_data)} episodes")
    
    print(f"\nâœ… Pass 3 complete!")
    print(f"   Total episodes: {len(all_episodes_data)}")
    print(f"   Total steps: {total_steps}")
    print(f"   Output path: {output_path}")
    
    # æ¨é€åˆ° Hub
    if push_to_hub:
        print(f"\nğŸ“¤ Pushing to Hugging Face Hub...")
        dataset.push_to_hub(
            tags=["libero", "panda", "rlds", "advanced", "value", "advantage"],
            private=False,
            push_videos=True,
            license="apache-2.0",
        )
        print(f"âœ… Successfully pushed to Hub!")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ All processing complete!")
    print("=" * 80)


if __name__ == "__main__":
    tyro.cli(main)
