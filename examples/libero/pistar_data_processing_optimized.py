"""
Pistar æ•°æ®å¤„ç†æµç¨‹è„šæœ¬ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

ä¼˜åŒ–è¦ç‚¹ï¼š
1. ä½¿ç”¨æµå¼å¤„ç†ï¼Œä¸å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜
2. é‡‡ç”¨ä¸¤éå¤„ç†ï¼š
   - Pass 1: è®¡ç®— epsilonï¼ˆåªä¿å­˜ advantages è½»é‡æ•°æ®ï¼‰
   - Pass 2: æµå¼å†™å…¥æ•°æ®é›†
3. ç›¸æ¯”åŸç‰ˆèŠ‚çœ 180GB+ å†…å­˜

å¤„ç†æµç¨‹ï¼š
- Pass 1: è®¡ç®—æ‰€æœ‰ episodes çš„ advantagesï¼Œç»Ÿè®¡ epsilonï¼ˆä¸ä¿å­˜å›¾åƒæ•°æ®ï¼‰
- Pass 2: é‡æ–°åŠ è½½æ•°æ®ï¼Œè¾¹è¯»è¾¹å†™ï¼Œä½¿ç”¨ epsilon è®¡ç®— adv_ind

æ³¨æ„ï¼š
- Value å–å€¼èŒƒå›´: [-1.0, 0.0]
- æ‰€æœ‰ value ä½¿ç”¨ --default_value å‚æ•°è®¾ç½®ï¼ˆé»˜è®¤ 0.0ï¼‰
- value_model_path åŠŸèƒ½å¾…å®ç°
- å¯é€šè¿‡ --default_adv_ind è·³è¿‡ adv è®¡ç®—ï¼Œç›´æ¥è®¾ç½®æ‰€æœ‰ adv_ind

Usage:
# è®¡ç®— adv å’Œ adv_indï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
python examples/libero/pistar_data_processing_optimized.py \
    --data_dir /path/to/modified_libero_rlds \
    --default_value 0.0 \
    --n_steps 10

# è·³è¿‡ adv è®¡ç®—ï¼Œç›´æ¥è®¾ç½® adv_ind
python examples/libero/pistar_data_processing_optimized.py \
    --data_dir /path/to/modified_libero_rlds \
    --default_adv_ind positive

# ä½¿ç”¨ value æ¨¡å‹ï¼ˆå¾…å®ç°ï¼‰
python examples/libero/pistar_data_processing_optimized.py \
    --data_dir /path/to/modified_libero_rlds \
    --value_model_path /path/to/value_model.pth \
    --n_steps 10

# unbuffered è¾“å‡ºæ—¥å¿—ï¼ˆå®æ—¶æŸ¥çœ‹ï¼‰
python -u examples/libero/pistar_data_processing_optimized.py ... 
"""

import os
# ç¦ç”¨ GPUï¼ˆæ­¤è„šæœ¬åªéœ€è¦ CPUï¼‰
os.environ['CUDA_VISIBLE_DEVICES'] = ''

import shutil
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple

from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME
from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
import tensorflow_datasets as tfds
import numpy as np
import tyro


REPO_NAME = "ybpy/libero_pistar"
RAW_DATASET_NAMES = [
    "libero_10_no_noops",
    "libero_goal_no_noops",
    "libero_object_no_noops",
    "libero_spatial_no_noops",
]


def transform_reward(original_reward: float, is_terminal: bool, is_last: bool, episode_length: int) -> float:
    """
    è½¬æ¢ reward çš„è§„åˆ™ï¼š
    1. å¦‚æœ is_terminal æˆ– is_last ä¸º True:
       - åŸå§‹ reward = 1.0 â†’ 0.0
       - åŸå§‹ reward = 0.0 â†’ -1.0
    2. å¦‚æœ is_terminal å’Œ is_last éƒ½ä¸º False (ä¸­é—´æ­¥éª¤):
       - reward = -1 / episode_length
    """
    if is_terminal or is_last:
        # è‡³å°‘ä¸€ä¸ªä¸º True
        if original_reward == 1.0:
            return 0.0
        else:  # original_reward == 0.0
            return -1.0
    else:
        # éƒ½ä¸º False (ä¸­é—´æ­¥éª¤): reward = -1 / episode_length
        return -1.0 / episode_length


def compute_value_placeholder(step_data: dict) -> float:
    """
    å ä½å‡½æ•°ï¼šè®¡ç®— value
    TODO: æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹æ¨ç†
    
    Args:
        step_data: åŒ…å« observation, action ç­‰çš„å­—å…¸
    
    Returns:
        value: é¢„æµ‹çš„çŠ¶æ€ä»·å€¼ (èŒƒå›´: [-1.0, 0.0])
    """
    # å½“å‰ä½¿ç”¨ 0.0 ä½œä¸ºé»˜è®¤å€¼
    # Value çš„å–å€¼èŒƒå›´é¢„å®šåœ¨ [-1.0, 0.0] ä¹‹é—´
    return 0.0


def compute_advantage(
    rewards: np.ndarray,
    values: np.ndarray,
    n_steps: int,
    gamma: float = 1.0
) -> np.ndarray:
    """
    è®¡ç®— advantage
    
    adv[t] = sum(rewards[t:t+N]) + value[t+N] - value[t]
    
    æ³¨æ„ï¼šå¯¹äº episode çš„æœ€å N ä¸ª stepï¼š
    - çª—å£ä¼šæˆªæ–­åˆ° episode ç»“æŸ
    - ä½¿ç”¨ episode æœ€åä¸€ä¸ª step çš„ value ä½œä¸º bootstrap
    - ä¾‹å¦‚ï¼št=95, T=100, N=10 æ—¶ï¼Œactual_steps=5
      adv[95] = sum(rewards[95:100]) + gamma^5 * value[99] - value[95]
    
    Args:
        rewards: shape (T,) çš„ reward æ•°ç»„
        values: shape (T,) çš„ value æ•°ç»„
        n_steps: N-step çª—å£å¤§å°
        gamma: æŠ˜æ‰£å› å­ (é»˜è®¤ 1.0)
    
    Returns:
        advantages: shape (T,) çš„ advantage æ•°ç»„
    """
    T = len(rewards)
    advantages = np.zeros(T, dtype=np.float32)
    
    for t in range(T):
        # è®¡ç®— N-step return (çª—å£ä¼šè‡ªåŠ¨æˆªæ–­åˆ° episode ç»“æŸ)
        n_step_return = 0.0
        actual_steps = min(n_steps, T - t)  # å®é™…èƒ½çœ‹åˆ°çš„æ­¥æ•°
        
        for i in range(actual_steps):
            n_step_return += (gamma ** i) * rewards[t + i]
        
        # æ·»åŠ  bootstrap value
        if t + n_steps < T:
            # æ­£å¸¸æƒ…å†µï¼šåŠ ä¸Š N æ­¥åçš„ value
            n_step_return += (gamma ** n_steps) * values[t + n_steps]
        else:
            # æœ€å N ä¸ª stepï¼šåŠ ä¸Š episode æœ€åä¸€ä¸ª step çš„ value
            n_step_return += (gamma ** actual_steps) * values[T - 1]
        
        # advantage = n_step_return - value[t]
        advantages[t] = n_step_return - values[t]
    
    return advantages


def main(
    data_dir: str,
    *,
    n_steps: int = 10,
    value_model_path: str | None = None,
    default_value: float = 0.0,
    default_adv_ind: str | None = None,
    epsilon_percentile: float = 70.0,
    repo_name: str = REPO_NAME,
    push_to_hub: bool = False,
):
    """
    Pistar æ•°æ®å¤„ç†å’Œè½¬æ¢ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        data_dir: RLDS æ•°æ®é›†è·¯å¾„
        n_steps: N-step advantage è®¡ç®—çš„çª—å£å¤§å°
        value_model_path: Value æ¨¡å‹è·¯å¾„ (å¯é€‰ï¼Œå¾…å®ç°)
        default_value: é»˜è®¤ value å€¼ (èŒƒå›´ [-1.0, 0.0]ï¼Œé»˜è®¤ 0.0)
        default_adv_ind: é»˜è®¤ adv_ind å€¼ ("positive" æˆ– "negative")ï¼Œ
                        å¦‚æœè®¾ç½®ï¼Œå°†è·³è¿‡ adv å’Œ epsilon è®¡ç®—
        epsilon_percentile: epsilon çš„åˆ†ä½æ•° (é»˜è®¤ 70.0)
        repo_name: è¾“å‡ºæ•°æ®é›†åç§°
        push_to_hub: æ˜¯å¦æ¨é€åˆ° HuggingFace Hub
    """
    
    print("=" * 80)
    print("ğŸš€ Pistar æ•°æ®å¤„ç†æµç¨‹ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
    print("=" * 80)
    print(f"N-step window: {n_steps}")
    print(f"Default value: {default_value}")
    if default_adv_ind:
        print(f"Default adv_ind: {default_adv_ind} (è·³è¿‡ adv è®¡ç®—)")
    else:
        print(f"Epsilon percentile: {epsilon_percentile}%")
    print(f"Output repo: {repo_name}")
    
    # æ£€æŸ¥ value æ¨¡å‹
    if value_model_path:
        print(f"âš ï¸  Value model loading not yet implemented")
        print(f"    Using default value: {default_value}")
    else:
        print(f"ğŸ“Œ Using default value: {default_value}")
    
    # ========================================================================
    # Pass 1: è®¡ç®— epsilonï¼ˆåªä¿å­˜è½»é‡çº§æ•°æ®ï¼šrewards/values/taskï¼‰
    # ========================================================================
    task_epsilon = {}
    
    if not default_adv_ind:
        print("\n" + "=" * 80)
        print("ğŸ“ˆ Pass 1: è®¡ç®— advantages å¹¶ç»Ÿè®¡ epsilonï¼ˆè½»é‡çº§æ‰«æï¼‰")
        print("=" * 80)
        
        # æŒ‰ task æ”¶é›†æ‰€æœ‰ advantages
        task_advantages = defaultdict(list)
        episode_count = 0
        
        for dataset_name in RAW_DATASET_NAMES:
            print(f"\nğŸ”„ Processing: {dataset_name}")
            raw_dataset = tfds.load(dataset_name, data_dir=data_dir, split="train")
            
            for episode in raw_dataset:
                steps_list = list(episode['steps'].as_numpy_iterator())
                episode_length = len(steps_list)
                
                # è·å– task
                task = steps_list[0]['language_instruction']
                task = task.decode() if isinstance(task, bytes) else task
                
                # åªæå–è®¡ç®—æ‰€éœ€çš„æœ€å°æ•°æ®ï¼ˆä¸ä¿å­˜ observationï¼‰
                rewards = []
                values = []
                
                for step in steps_list:
                    original_reward = float(step['reward'])
                    is_terminal = bool(step['is_terminal'])
                    is_last = bool(step['is_last'])
                    
                    transformed_reward = transform_reward(
                        original_reward, is_terminal, is_last, episode_length
                    )
                    
                    # è·å–æˆ–è®¡ç®— value
                    if value_model_path:
                        # TODO: ä½¿ç”¨å®é™…æ¨¡å‹è®¡ç®— value
                        value = compute_value_placeholder(step)
                    else:
                        value = default_value
                    
                    rewards.append(transformed_reward)
                    values.append(value)
                
                # è®¡ç®— advantages
                rewards_array = np.array(rewards, dtype=np.float32)
                values_array = np.array(values, dtype=np.float32)
                advantages = compute_advantage(rewards_array, values_array, n_steps)
                
                # æ”¶é›†åˆ° task_advantages ä¸­
                task_advantages[task].extend(advantages.tolist())
                
                episode_count += 1
                if episode_count % 50 == 0:
                    print(f"   Processed {episode_count} episodes")
        
        # è®¡ç®—æ¯ä¸ª task çš„ epsilon (åŸºäº advantages)
        print(f"\nğŸ“Š Computing epsilon for {len(task_advantages)} unique tasks")
        for task, advantages in task_advantages.items():
            epsilon = np.percentile(advantages, epsilon_percentile)
            task_epsilon[task] = epsilon
            print(f"Task: {task[:50]}...")
            print(f"  Advantages count: {len(advantages)}")
            print(f"  Epsilon ({epsilon_percentile}%): {epsilon:.4f}")
        
        print(f"\nâœ… Pass 1 complete: {episode_count} episodes scanned")
    else:
        print("\n" + "=" * 80)
        print(f"â­ï¸  Pass 1: è·³è¿‡ (ä½¿ç”¨é»˜è®¤ adv_ind: {default_adv_ind})")
        print("=" * 80)
    
    # ========================================================================
    # Pass 2: æµå¼å†™å…¥æ•°æ®é›†ï¼ˆè¾¹è¯»è¾¹å†™ï¼Œä¸åœ¨å†…å­˜ä¸­ç§¯ç´¯ï¼‰
    # ========================================================================
    print("\n" + "=" * 80)
    print("ğŸ’¾ Pass 2: æµå¼å†™å…¥æ•°æ®é›†ï¼ˆå†…å­˜å‹å¥½æ¨¡å¼ï¼‰")
    print("=" * 80)
    
    # æ¸…ç†å·²å­˜åœ¨çš„æ•°æ®é›†
    output_path = HF_LEROBOT_HOME / repo_name
    if output_path.exists():
        print(f"ğŸ—‘ï¸  Removing existing dataset at {output_path}")
        shutil.rmtree(output_path)
    
    # åˆ›å»º LeRobot æ•°æ®é›†
    dataset = LeRobotDataset.create(
        repo_id=repo_name,
        robot_type="panda",
        fps=10,
        features={
            "image": {
                "dtype": "image",
                "shape": (256, 256, 3),
                "names": ["height", "width", "channel"],
            },
            "wrist_image": {
                "dtype": "image",
                "shape": (256, 256, 3),
                "names": ["height", "width", "channel"],
            },
            "state": {
                "dtype": "float32",
                "shape": (8,),
                "names": ["state"],
            },
            "actions": {
                "dtype": "float32",
                "shape": (7,),
                "names": ["actions"],
            },
            "reward": {
                "dtype": "float32",
                "shape": (1,),
                "names": ["reward"],
            },
            "value": {
                "dtype": "float32",
                "shape": (1,),
                "names": ["value"],
            },
            "adv": {
                "dtype": "float32",
                "shape": (1,),
                "names": ["adv"],
            },
            "epsilon": {
                "dtype": "float32",
                "shape": (1,),
                "names": ["epsilon"],
            },
            "adv_ind": {
                "dtype": "string",
                "shape": (1,),
                "names": ["adv_ind"],
            },
        },
        image_writer_threads=10,
        image_writer_processes=5,
    )
    
    # é‡æ–°æ‰«ææ•°æ®é›†ï¼Œè¾¹è¯»è¾¹å†™
    total_steps = 0
    episode_count = 0
    
    for dataset_name in RAW_DATASET_NAMES:
        print(f"\nğŸ”„ Processing: {dataset_name}")
        raw_dataset = tfds.load(dataset_name, data_dir=data_dir, split="train")
        
        for episode in raw_dataset:
            steps_list = list(episode['steps'].as_numpy_iterator())
            episode_length = len(steps_list)
            
            # è·å– task
            task = steps_list[0]['language_instruction']
            task = task.decode() if isinstance(task, bytes) else task
            
            # è®¡ç®—æœ¬ episode çš„ advantages å’Œ epsilon
            if default_adv_ind:
                # ä½¿ç”¨é»˜è®¤ adv_indï¼Œè·³è¿‡ adv è®¡ç®—
                advantages = np.zeros(episode_length, dtype=np.float32)
                epsilon = 0.0
            else:
                # é‡æ–°è®¡ç®— advantagesï¼ˆè½»é‡çº§æ“ä½œï¼‰
                rewards = []
                values = []
                
                for step in steps_list:
                    original_reward = float(step['reward'])
                    is_terminal = bool(step['is_terminal'])
                    is_last = bool(step['is_last'])
                    
                    transformed_reward = transform_reward(
                        original_reward, is_terminal, is_last, episode_length
                    )
                    
                    if value_model_path:
                        value = compute_value_placeholder(step)
                    else:
                        value = default_value
                    
                    rewards.append(transformed_reward)
                    values.append(value)
                
                rewards_array = np.array(rewards, dtype=np.float32)
                values_array = np.array(values, dtype=np.float32)
                advantages = compute_advantage(rewards_array, values_array, n_steps)
                epsilon = task_epsilon[task]
            
            # å†™å…¥æ¯ä¸ª step
            for step_idx, step in enumerate(steps_list):
                original_reward = float(step['reward'])
                is_terminal = bool(step['is_terminal'])
                is_last = bool(step['is_last'])
                
                transformed_reward = transform_reward(
                    original_reward, is_terminal, is_last, episode_length
                )
                
                if value_model_path:
                    value = compute_value_placeholder(step)
                else:
                    value = default_value
                
                adv = advantages[step_idx]
                
                if default_adv_ind:
                    adv_ind = default_adv_ind
                else:
                    adv_ind = "positive" if adv > epsilon else "negative"
                
                dataset.add_frame({
                    "image": step['observation']['image'],
                    "wrist_image": step['observation']['wrist_image'],
                    "state": step['observation']['state'],
                    "actions": step['action'],
                    "task": task,
                    "reward": np.array([transformed_reward], dtype=np.float32),
                    "value": np.array([value], dtype=np.float32),
                    "adv": np.array([adv], dtype=np.float32),
                    "epsilon": np.array([epsilon], dtype=np.float32),
                    "adv_ind": adv_ind,
                })
                total_steps += 1
            
            dataset.save_episode()
            episode_count += 1
            
            if episode_count % 50 == 0:
                print(f"   Written {episode_count} episodes")
    
    print(f"\nâœ… Pass 2 complete!")
    print(f"   Total episodes: {episode_count}")
    print(f"   Total steps: {total_steps}")
    print(f"   Output path: {output_path}")
    
    # æ¨é€åˆ° Hub
    if push_to_hub:
        print(f"\nğŸ“¤ Pushing to Hugging Face Hub...")
        dataset.push_to_hub(
            tags=["libero", "panda", "rlds", "advanced", "value", "advantage"],
            private=False,
            push_videos=True,
            license="apache-2.0",
        )
        print(f"âœ… Successfully pushed to Hub!")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ All processing complete!")
    print("=" * 80)


if __name__ == "__main__":
    tyro.cli(main)
