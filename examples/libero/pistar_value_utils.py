"""
å·¥å…·è„šæœ¬ï¼šç”¨äºä¿å­˜å’ŒåŠ è½½é¢„è®¡ç®—çš„ value å€¼

å½“ä½ æœ‰äº†è®­ç»ƒå¥½çš„ value æ¨¡å‹åï¼Œä½¿ç”¨è¿™ä¸ªè„šæœ¬ï¼š
1. è¿‡ä¸€éæ‰€æœ‰æ•°æ®ï¼Œç”¨æ¨¡å‹è®¡ç®— value
2. ä¿å­˜ä¸º .npz æ–‡ä»¶
3. åœ¨è½¬æ¢è„šæœ¬ä¸­åŠ è½½ä½¿ç”¨
"""

import numpy as np
import tensorflow_datasets as tfds
from pathlib import Path
import tyro


def compute_and_save_values(
    data_dir: str,
    output_path: str,
    value_model_path: str | None = None,
    use_random: bool = False,
):
    """
    è®¡ç®—å¹¶ä¿å­˜æ‰€æœ‰æ•°æ®çš„ value
    
    Args:
        data_dir: RLDS æ•°æ®é›†è·¯å¾„
        output_path: è¾“å‡ºçš„ .npz æ–‡ä»¶è·¯å¾„
        value_model_path: Value æ¨¡å‹è·¯å¾„
        use_random: æ˜¯å¦ä½¿ç”¨éšæœºå€¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    """
    print("=" * 80)
    print("ğŸ’¾ è®¡ç®—å¹¶ä¿å­˜ Value å€¼")
    print("=" * 80)
    
    if use_random:
        print("âš ï¸  ä½¿ç”¨éšæœºå€¼ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
    elif value_model_path:
        print(f"ğŸ“¦ åŠ è½½ value æ¨¡å‹: {value_model_path}")
        # TODO: åŠ è½½å®é™…æ¨¡å‹
        # model = load_value_model(value_model_path)
        print("âš ï¸  æ¨¡å‹åŠ è½½åŠŸèƒ½å¾…å®ç°ï¼Œæš‚æ—¶ä½¿ç”¨éšæœºå€¼")
        use_random = True
    else:
        print("âŒ è¯·æä¾› --value_model_path æˆ–ä½¿ç”¨ --use_random")
        return
    
    dataset_names = [
        "libero_10_no_noops",
        "libero_goal_no_noops",
        "libero_object_no_noops",
        "libero_spatial_no_noops",
    ]
    
    episode_indices = []
    step_indices = []
    values = []
    
    global_episode_idx = 0
    
    for dataset_name in dataset_names:
        print(f"\nğŸ”„ å¤„ç†: {dataset_name}")
        raw_dataset = tfds.load(dataset_name, data_dir=data_dir, split="train")
        
        for episode in raw_dataset:
            steps_list = list(episode['steps'].as_numpy_iterator())
            
            for step_idx, step in enumerate(steps_list):
                # è®¡ç®— value
                if use_random:
                    value = float(np.random.randn())
                else:
                    # TODO: ä½¿ç”¨å®é™…æ¨¡å‹
                    # value = model.predict(step['observation'])
                    value = 0.0
                
                episode_indices.append(global_episode_idx)
                step_indices.append(step_idx)
                values.append(value)
            
            global_episode_idx += 1
            
            if global_episode_idx % 50 == 0:
                print(f"   å¤„ç†äº† {global_episode_idx} episodes, {len(values)} steps")
    
    # ä¿å­˜
    episode_indices = np.array(episode_indices, dtype=np.int32)
    step_indices = np.array(step_indices, dtype=np.int32)
    values = np.array(values, dtype=np.float32)
    
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    np.savez(
        output_file,
        episode_indices=episode_indices,
        step_indices=step_indices,
        values=values,
    )
    
    print(f"\nâœ… ä¿å­˜å®Œæˆ!")
    print(f"   æ–‡ä»¶: {output_file}")
    print(f"   Episodes: {global_episode_idx}")
    print(f"   Total steps: {len(values)}")
    print(f"   æ–‡ä»¶å¤§å°: {output_file.stat().st_size / 1024 / 1024:.2f} MB")


def load_and_inspect_values(values_path: str):
    """
    åŠ è½½å¹¶æ£€æŸ¥ä¿å­˜çš„ value æ–‡ä»¶
    
    Args:
        values_path: .npz æ–‡ä»¶è·¯å¾„
    """
    print("=" * 80)
    print("ğŸ” æ£€æŸ¥ Value æ–‡ä»¶")
    print("=" * 80)
    
    data = np.load(values_path)
    
    episode_indices = data['episode_indices']
    step_indices = data['step_indices']
    values = data['values']
    
    print(f"\næ–‡ä»¶: {values_path}")
    print(f"æ€»æ•°æ®ç‚¹: {len(values)}")
    print(f"Episodes: {episode_indices.max() + 1}")
    
    print(f"\nValue ç»Ÿè®¡:")
    print(f"  å‡å€¼: {values.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {values.std():.4f}")
    print(f"  æœ€å°å€¼: {values.min():.4f}")
    print(f"  æœ€å¤§å€¼: {values.max():.4f}")
    print(f"  ä¸­ä½æ•°: {np.median(values):.4f}")
    
    print(f"\nå‰10ä¸ªæ•°æ®ç‚¹:")
    for i in range(min(10, len(values))):
        print(f"  Episode {episode_indices[i]}, Step {step_indices[i]}: value = {values[i]:.4f}")
    
    print("\nâœ… æ£€æŸ¥å®Œæˆ!")


import sys

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Pistar Value è®¡ç®—å·¥å…·")
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")
    
    # compute å­å‘½ä»¤
    parser_compute = subparsers.add_parser("compute", help="è®¡ç®—å¹¶ä¿å­˜ value å€¼")
    parser_compute.add_argument("--data-dir", type=str, required=True, help="RLDS æ•°æ®é›†è·¯å¾„")
    parser_compute.add_argument("--output-path", type=str, required=True, help="è¾“å‡ºçš„ .npz æ–‡ä»¶è·¯å¾„")
    parser_compute.add_argument("--value-model-path", type=str, default=None, help="Value æ¨¡å‹è·¯å¾„")
    parser_compute.add_argument("--use-random", action="store_true", help="æ˜¯å¦ä½¿ç”¨éšæœºå€¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    
    # inspect å­å‘½ä»¤
    parser_inspect = subparsers.add_parser("inspect", help="æ£€æŸ¥ä¿å­˜çš„ value æ–‡ä»¶")
    parser_inspect.add_argument("--values-path", type=str, required=True, help=".npz æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    if args.command == "compute":
        compute_and_save_values(
            data_dir=args.data_dir,
            output_path=args.output_path,
            value_model_path=args.value_model_path,
            use_random=args.use_random,
        )
    elif args.command == "inspect":
        load_and_inspect_values(values_path=args.values_path)
    else:
        parser.print_help()
